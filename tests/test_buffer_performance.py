#!/usr/bin/env python3
"""Performance test for buffer service with large dataset."""

import sys
import tempfile
import time
from pathlib import Path
from datetime import datetime, timedelta

# Add the app directory to the Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.database.buffer_service import BufferService, FilterCriteria
from app.database.service import DatabaseService
from app.database.engine import init_database


def create_large_test_database(num_items=10000):
    """Create a test database with a large number of sample items."""
    tmpdir = tempfile.mkdtemp()
    tmppath = Path(tmpdir)
    
    # Initialize SQLite database
    db_path = tmppath / "large_test.db"
    db_url = f"sqlite:///{db_path}"
    
    print(f"Creating test database with {num_items} items at {db_path}")
    init_database(db_url)
    
    # Create sample media files in batches
    batch_size = 1000
    start_time = time.time()
    
    with DatabaseService() as db:
        for i in range(num_items):
            file_path = tmppath / f"test_image_{i:05d}.jpg"
            file_path.touch()  # Create empty file
            
            media_file = db.get_or_create_media_file(file_path)
            media_file.score = (i % 6)  # Scores 0-5
            media_file.file_type = "image"
            media_file.extension = ".jpg"
            media_file.file_size = 1024 * (i + 1)
            
            # Vary creation dates
            media_file.original_created_at = datetime.utcnow() - timedelta(days=i % 365)
            
            # Add keywords to a subset
            if i % 10 == 0:
                db.add_keywords(file_path, ["landscape"], keyword_type="auto")
            if i % 15 == 0:
                db.add_keywords(file_path, ["portrait"], keyword_type="auto")
            if i % 20 == 0:
                db.add_keywords(file_path, ["abstract"], keyword_type="auto")
            
            # Progress indicator
            if (i + 1) % batch_size == 0:
                elapsed = time.time() - start_time
                rate = (i + 1) / elapsed
                remaining = (num_items - i - 1) / rate
                print(f"  Progress: {i + 1}/{num_items} ({(i+1)/num_items*100:.1f}%) - "
                      f"{rate:.0f} items/sec - ETA: {remaining:.1f}s")
    
    total_time = time.time() - start_time
    print(f"âœ… Created {num_items} items in {total_time:.2f}s ({num_items/total_time:.0f} items/sec)")
    
    return tmppath, db_url


def test_large_dataset_performance():
    """Test buffer service performance with large dataset."""
    print("\nðŸ”¬ Performance Test: Large Dataset (10,000 items)")
    print("=" * 70)
    
    # Create large test database
    tmppath, db_url = create_large_test_database(10000)
    
    # Initialize buffer service
    buffer_db_path = tmppath / "buffer.db"
    buffer_service = BufferService(buffer_db_path)
    
    print("\nTest 1: Initial buffer creation (all items)")
    print("-" * 70)
    
    filters = FilterCriteria(
        sort_field="date",
        sort_direction="desc"
    )
    
    start_time = time.time()
    with DatabaseService() as db:
        filter_hash, item_count = buffer_service.get_or_create_buffer(filters, db)
    creation_time = time.time() - start_time
    
    print(f"âœ… Buffer created in {creation_time:.3f}s")
    print(f"   Filter hash: {filter_hash[:16]}")
    print(f"   Items in buffer: {item_count}")
    print(f"   Performance: {item_count/creation_time:.0f} items/sec")
    
    print("\nTest 2: Buffer reuse (should be instant)")
    print("-" * 70)
    
    start_time = time.time()
    with DatabaseService() as db:
        filter_hash2, item_count2 = buffer_service.get_or_create_buffer(filters, db)
    reuse_time = time.time() - start_time
    
    print(f"âœ… Buffer reused in {reuse_time:.3f}s (should be < 0.01s)")
    assert reuse_time < 0.1, "Buffer reuse should be very fast"
    
    print("\nTest 3: Pagination performance (100 pages of 100 items)")
    print("-" * 70)
    
    cursor = None
    pages_fetched = 0
    total_items_fetched = 0
    
    start_time = time.time()
    for page_num in range(100):
        items, cursor = buffer_service.get_page(filter_hash, cursor, limit=100)
        pages_fetched += 1
        total_items_fetched += len(items)
        
        if not cursor:
            break
    
    pagination_time = time.time() - start_time
    avg_page_time = pagination_time / pages_fetched
    
    print(f"âœ… Fetched {pages_fetched} pages ({total_items_fetched} items) in {pagination_time:.3f}s")
    print(f"   Average page fetch time: {avg_page_time*1000:.2f}ms")
    print(f"   Performance: {total_items_fetched/pagination_time:.0f} items/sec")
    
    # Verify performance is acceptable for mobile
    assert avg_page_time < 0.05, "Page fetch should be < 50ms for smooth scrolling"
    
    print("\nTest 4: Filtered buffer creation (high scores only)")
    print("-" * 70)
    
    filtered_criteria = FilterCriteria(
        min_score=4,
        sort_field="date",
        sort_direction="desc"
    )
    
    start_time = time.time()
    with DatabaseService() as db:
        filtered_hash, filtered_count = buffer_service.get_or_create_buffer(filtered_criteria, db)
    filtered_time = time.time() - start_time
    
    print(f"âœ… Filtered buffer created in {filtered_time:.3f}s")
    print(f"   Items in buffer: {filtered_count}")
    print(f"   Performance: {filtered_count/filtered_time:.0f} items/sec")
    
    print("\nTest 5: Keyword search buffer")
    print("-" * 70)
    
    keyword_criteria = FilterCriteria(
        keywords=["landscape"],
        sort_field="date"
    )
    
    start_time = time.time()
    with DatabaseService() as db:
        keyword_hash, keyword_count = buffer_service.get_or_create_buffer(keyword_criteria, db)
    keyword_time = time.time() - start_time
    
    print(f"âœ… Keyword buffer created in {keyword_time:.3f}s")
    print(f"   Items in buffer: {keyword_count}")
    print(f"   Expected items: ~{10000 // 10} (every 10th item has 'landscape')")
    
    print("\nTest 6: Buffer statistics")
    print("-" * 70)
    
    stats = buffer_service.get_buffer_stats()
    print(f"   Total buffers: {stats['buffer_count']}")
    print(f"   Total items across buffers: {stats['total_items']}")
    print(f"   Total buffer size: {stats['total_size_mb']:.2f} MB")
    print(f"   Average buffer size: {stats['total_size_mb']/stats['buffer_count']:.2f} MB")
    
    print("\n" + "=" * 70)
    print("ðŸŽ‰ Performance Test Summary")
    print("-" * 70)
    print(f"âœ… Buffer creation: {item_count/creation_time:.0f} items/sec")
    print(f"âœ… Buffer reuse: {reuse_time*1000:.2f}ms (instant)")
    print(f"âœ… Page fetch: {avg_page_time*1000:.2f}ms avg")
    print(f"âœ… Pagination throughput: {total_items_fetched/pagination_time:.0f} items/sec")
    print(f"âœ… Total buffers: {stats['buffer_count']}")
    print(f"âœ… Storage used: {stats['total_size_mb']:.2f} MB")
    print("\nâœ… All performance targets met!")
    print("   - Buffer creation < 1 second per 1000 items âœ“")
    print("   - Page fetch < 50ms for smooth mobile scrolling âœ“")
    print("   - Buffer reuse is instant âœ“")


if __name__ == "__main__":
    try:
        test_large_dataset_performance()
        
    except AssertionError as e:
        print(f"\nâŒ Performance test failed: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
